{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "154c1d48-aa69-4430-91d3-c170843c84d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be958bd-2ca3-42d8-acae-2ae4f38fb643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_proba(model, X_dataframe):\n",
    "    y_pred_proba = model.predict_proba(X_dataframe)[:, 1]\n",
    "    return y_pred_proba\n",
    "\n",
    "\n",
    "def get_predictions_test(model, X_test, threshold=0.5):\n",
    "    y_pred_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred_test = np.array([1 if p > threshold else 0 for p in y_pred_test_proba])\n",
    "    return y_pred_test\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, threshold=0.5):\n",
    "    model_name = type(model).__name__\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "\n",
    "    # Train set\n",
    "    y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "    auc_train = roc_auc_score(y_train, y_pred_proba_train)\n",
    "    y_pred_train = np.array([1 if p > threshold else 0 for p in y_pred_proba_train])\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    precision_train = precision_score(y_train, y_pred_train)\n",
    "    recall_train = recall_score(y_train, y_pred_train)\n",
    "    f1_train = f1_score(y_train, y_pred_train)\n",
    "\n",
    "    # Test set\n",
    "    y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "    auc_test = roc_auc_score(y_test, y_pred_proba_test)\n",
    "    y_pred_test = np.array([1 if p > threshold else 0 for p in y_pred_proba_test])\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    precision_test = precision_score(y_test, y_pred_test)\n",
    "    recall_test = recall_score(y_test, y_pred_test)\n",
    "    f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "    # Create DataFrame\n",
    "    data = {\n",
    "        'Model': [model_name, model_name],\n",
    "        'Dataset': ['Train', 'Test'],\n",
    "        'AUC': [auc_train, auc_test],\n",
    "        'Accuracy': [accuracy_train, accuracy_test],\n",
    "        'Precision': [precision_train, precision_test],\n",
    "        'Recall': [recall_train, recall_test],\n",
    "        'F1-score': [f1_train, f1_test]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def specificity_score(y_true, y_pred):\n",
    "    tn = ((y_true == 0) & (y_pred == 0)).sum()  # True negatives\n",
    "    fp = ((y_true == 0) & (y_pred == 1)).sum()  # False positives\n",
    "\n",
    "    score = tn / (tn + fp)\n",
    "    return score\n",
    "    \n",
    "def confusion_matrices(model, X_train, y_train, X_test, y_test, threshold=0.5):\n",
    "    model_name = type(model).__name__\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "\n",
    "    # Train set\n",
    "    y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "    y_pred_train = np.array([1 if p > threshold else 0 for p in y_pred_proba_train])\n",
    "    cm = confusion_matrix(y_train, y_pred_train)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    disp.ax_.set_title(\"Train set\")\n",
    "    plt.show()\n",
    "\n",
    "    # Oot set\n",
    "    y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred_test = np.array([1 if p > threshold else 0 for p in y_pred_proba_test])\n",
    "    cm = confusion_matrix(y_test, y_pred_test)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    disp.ax_.set_title(\"Test set\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b89c44-8e61-4335-831b-37052d8be80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantum_wrapper:\n",
    "    def __init__(self, ):\n",
    "        self.model = None\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "       \n",
    "        self.model = LogisticRegression()\n",
    "        self.model.fit(X_train, y_train)\n",
    "    \n",
    "    def predict_proba(self, X_test):\n",
    "        \"\"\"\n",
    "        Predict class probabilities for input samples.\n",
    "        \n",
    "        Parameters:\n",
    "        - X_test: array-like, shape (n_samples, n_features)\n",
    "            The input samples for which to predict probabilities.\n",
    "        \n",
    "        Returns:\n",
    "        - proba: array-like, shape (n_samples, n_classes)\n",
    "            The class probabilities of the input samples.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been trained. Please fit the model first.\")\n",
    "        return self.model.predict_proba(X_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base (qiskit)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
